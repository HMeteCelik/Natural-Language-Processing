# ğŸ“° News Article Enrichment with BERT & Google Gemini

This project provides an end-to-end solution for enhancing news articles by automatically classifying them using a fine-tuned BERT model and generating refined headlines and polished content using Google Gemini's LLM API.

---

ğŸ“Œ Project Overview

The goal is to streamline news content processing for online publications. The workflow includes:

* **ğŸ§  Category Classification** â€“ Predicting the category of a news article using a BERT-based model.
* **âœï¸ Title & Article Generation** â€“ Leveraging Google Gemini (or Gemma) to create a professional title and rewrite the article content.

Both tasks are seamlessly integrated into a single pipeline for automated processing.

---


ğŸ› ï¸ Tools & Requirements

* **Language**: Python
* **Environment**: Jupyter Notebook (ğŸ’¡ Google Colab with T4 GPU recommended)
* **Classifier**: BERT Base Uncased from HuggingFace's transformers
* **LLM Service**: Google Gemini (or Gemma API via Google AI Studio)

---

ğŸ“ Project Structure

The project consists of three main sections implemented in a single Jupyter Notebook:

### 1ï¸âƒ£ Category Classification

* **ğŸ” EDA**: Visualizing class distributions and text lengths.
* **ğŸ§¼ Preprocessing**: Cleaning data if needed.
* **ğŸ§ª Model Training**: Fine-tuning BERT with experimentation on hyperparameters (learning rate, epochs, etc.).
* **ğŸ“Š Evaluation**: Calculating F1 score (expected > 0.90) using `test.csv`.
* **ğŸ” Prediction Examples**: Predicting and displaying results for the first 5 test samples.
* **ğŸ’¾ Model Saving**: Storing the trained model to skip retraining.

### 2ï¸âƒ£ Title & Article Generation (LLM)

* **ğŸ”‘ API Integration**: Authenticate using Google Gemini API key.
* **ğŸ§  Prompt Engineering**: Structure prompts to yield clean and parsable output (avoid fluff like â€œHereâ€™s your title:â€).
* **ğŸ“¤ Output Extraction**: Programmatically extract the headline and rewritten article from the LLM response.

### 3ï¸âƒ£ End-to-End Pipeline Integration

* **ğŸ“ˆ Category Prediction**: Use the saved BERT model to predict and convert labels to category names.
* **âœï¸ LLM Enrichment**: Feed the article into the LLM to get the enhanced title and content.
* **ğŸ—‚ï¸ Output Formatting**: Return results in dictionary format:

    ```json
    {
      "category": "...",
      "title": "...",
      "article": "..."
    }
    ```

* **ğŸ§ª Pipeline Demo**: Process the last 5 samples from `train.csv` and print enriched results as a list of dictionaries.

---

ğŸš€ Quick Start

1.  Clone the repo and open the notebook in Google Colab.
2.  Upload the AG News subset (`train.csv`, `test.csv`).
3.  Fine-tune BERT or load the saved model.
4.  Set up your Google Gemini API key.
5.  Run the end-to-end pipeline.

