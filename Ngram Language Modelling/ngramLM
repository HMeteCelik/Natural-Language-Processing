# -*- coding: utf-8 -*-
"""
Created on Sat Mar  8 15:06:51 2025

@author: ilyas
"""

import math
import random
import re
import codecs


class ngramLM:
    """Ngram Language Model Class"""
    
    def __init__(self):
        self.numOfTokens = 0
        self.sizeOfVocab = 0
        self.numOfSentences = 0
        self.sentences = []
        self.words = []
        self.vocabList = []
        self.bigramList = [] 


    def trainFromFile(self, fn):
        file = open(fn, 'r',encoding="utf-8")
        lines = file.readlines()

        lines = [l.strip() for l in lines if l.strip()]
        

        for line in lines:
            tokens = self.tokenize_lines(line)  
            if tokens:  
                self.words.extend(tokens)
        
        self.numOfTokens = len(self.words)
        
        self.sizeOfVocab = len(set(self.words))

        self.sentences = self.sentence_finder(self.words)
        self.numOfSentences = len(self.sentences)
        
    
    def vocab(self):


        words = self.words.copy()

        i = 0
        while i < len(words):
            word = words[i]
            count = words.count(word)
            
            self.vocabList.append((word, count))


            words = [w for w in words if w != word]
            

        self.vocabList = sorted(self.vocabList, key=lambda item: (-item[1], item[0]), reverse=False)
        
        return self.vocabList
    

    def bigram_finder(self,key):
        count = 0
        for sentence in self.sentences:
            for ind in range(len(sentence) - 1):
                if (sentence[ind], sentence[ind+1]) == key:
                    count += 1
        return (key, count)
    

    def bigrams(self):
        
        self.bigramList = []
        keysDone = []
        for sentence in self.sentences:
            for ind in range(len(sentence) - 1):
                key = (sentence[ind], sentence[ind+1])
                if key not in keysDone:
                    self.bigramList.append(self.bigram_finder(key))
                    keysDone.append(key)  
        self.bigramList = sorted(self.bigramList, key=lambda item: (-item[1], item[0]), reverse=False)
        return  self.bigramList
    

    def unigramCount(self, word):
        
        for uni in self.vocab():
            if word == uni[0]:
                return uni[1]
        
        return 0
    

    def bigramCount(self, bigram):
        for bi in self.bigrams():
            if bigram == bi[0]:
                return bi[1]
        
        return 0
    

    def unigramProb(self, word):

        for uni in self.vocab():
            if word == uni[0]:
                return uni[1] / self.numOfTokens

        return 0 
    

    def bigramProb(self, bigram):
        if self.bigramCount(bigram) == 0:
            return 0
        return self.bigramCount(bigram) / self.unigramCount(bigram[0])
    

    def unigramProb_SmoothingUNK(self, word):

        numerator = self.unigramCount(word) + 1
        denominator = self.numOfTokens + self.sizeOfVocab + 1 
        
        return numerator / denominator 
    

    def bigramProb_SmoothingUNK(self, bigram):

        numerator = self.bigramCount(bigram) + 1 
        denominator = self.unigramCount(bigram[0]) + self.sizeOfVocab + 1
        return numerator / denominator
    

    def sentenceProb(self,sent):
        
        prob = 1

        if len(sent) == 1:
            return self.unigramProb_SmoothingUNK(sent[0])
        for ind in range(len(sent) - 1):
            prob *= self.bigramProb_SmoothingUNK((sent[ind],sent[ind + 1]))
        
        return prob
    

    def genSentence(self, word, maxFollowWords):
        count = 0
        pairs = []
        for keys in self.bigrams():
            if count == maxFollowWords:
                break

            if keys[0][0] == word:
                pairs += [keys[0]] * keys[1]
                count += 1
        chosen = random.choice(pairs)
        return chosen[1]
    

    def generateSentence(self,sent=["<s>"],maxFollowWords=1,maxWordsInSent=20):
        count = 0
        while sent[-1] != '</s>' and count < maxWordsInSent:
            sent.append(self.genSentence(sent[-1],maxFollowWords))
        if sent[-1] != '</s>':
            sent.append('</s>')

        return sent
    

    def sentence_finder(self,all_tokens):
        start = 0
        sentences = []
        for i in range(len(all_tokens)):
            if all_tokens[i] == '</s>':
                sentences.append(all_tokens[start:i+1])
                start = i + 1

        return sentences


    def tokenize_lines(self, line):
        line = line.replace("I", "ı").replace("İ", "i").lower()
        

        sentences = re.split(r'(?<!\d)([.?!])', line)    


        combined_sentences = []
        if len(sentences) == 1:
            combined_sentences.append(sentences[0])
        else:
            for i in range(0, len(sentences)-1, 2):  
                sentence = sentences[i].strip()
                punctuation = sentences[i+1].strip() if i+1 < len(sentences) else ""
                if sentence:
                    combined_sentences.append(sentence + punctuation)


        tokens = []

        for sentence in combined_sentences:
            sentence_tokens = re.findall(r"""(?x)
                                            (?:[A-ZÇĞIİÖŞÜ]\.)+   
                                            | \d+(?:\.\d*)?(?:\'\w+)?   
                                            | \w+(?:-\w+)*(?:\'\w+)?   
                                            | \.\.\.   
                                            | [][,;.?():_!#^+$%&><|/{()=}\"\'\\\"\`-]   
                                        """, sentence)
            
            
            
            sentence_tokens.insert(0, "<s>")
            sentence_tokens.append("</s>")
            
            tokens.extend(sentence_tokens)
        
        return tokens
    
if __name__ == "__main__":
    lm = ngramLM()  
    lm.trainFromFile("hw02_bilgisayar.txt")

    with open("output3.txt", "w", encoding="utf-8") as f:

        print(lm.numOfTokens, file=f)
        print(lm.sizeOfVocab, file=f)
        print(lm.numOfSentences, file=f)

        print(lm.sentences, file=f)
        print(lm.vocab()[:100], file=f)
        print(lm.bigrams()[:100], file=f)

        print(lm.unigramCount('bir'), file=f)
        print(lm.unigramCount('bilgisayar'), file=f)

        print(lm.unigramProb('bir'), file=f)
        print(lm.unigramProb('bilgisayar'), file=f)

        print(lm.bigramCount(('bir','bilgisayar')), file=f)
        print(lm.bigramCount(('bilgisayar','bir')), file=f)

        print(lm.bigramProb(('bir','bilgisayar')), file=f)
        print(lm.bigramProb(('bilgisayar','bir')), file=f)

        print(lm.unigramProb_SmoothingUNK('bir'), file=f)
        print(lm.unigramProb_SmoothingUNK('bilgisayar'), file=f)

        print(lm.bigramProb_SmoothingUNK(('bir','bilgisayar')), file=f)
        print(lm.bigramProb_SmoothingUNK(('bilgisayar','bir')), file=f)

        print(lm.sentenceProb(['<s>', 'bilgisayar', 'bir', 'dizi', 'mantık', 'işlemini', 'otomatik', 'olarak', 'yapabilen', 'bir', 'makinedir', '.', '</s>']), file=f)
